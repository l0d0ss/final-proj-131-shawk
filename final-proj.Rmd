---
title: "PSTAT 131 Final Project"
author: "Katlyn Shaw"
date: "2022-11-28"
output: 
  html_document:
      toc: true
      toc_float: true
      code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
# load packages
library(glmnet)
library(janitor)
library(corrplot)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(tidyverse)
library(tidymodels)
library(ranger)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
library(yardstick)
library(zoo)
tidymodels_prefer()
```

```{r}

# load data
seoul_bike <- read.csv("/Users/katlynshaw/Desktop/pstat 131/final-proj-131-shawk/SeoulBikeDataColNameFIXED.csv") %>% 
  clean_names() %>% 
  filter(functioning_day == "Yes") %>% 
  mutate(date = as.Date(date, "%d/%m/%Y"),
         humidity = humidity / 100,
         time_of_day = factor(ifelse(hour < 6, "Night", ifelse(hour < 12, "Morning", ifelse(hour < 18, "Afternoon", ifelse(hour < 24, "Evening", "Null")))), levels = c("Night", "Morning", "Afternoon", "Evening")),
         hour = factor(hour, levels = c(0:23)),
         seasons = factor(seasons, levels = c("Winter", "Spring", "Summer", "Autumn")),
         holiday = factor(holiday, levels = c("Holiday", "No Holiday")),
         functioning_day = factor(functioning_day, levels = c("Yes", "No")),
         weekday = lubridate::wday(date, label = TRUE),
         workday = factor(ifelse(weekday != "Sun" & weekday != "Sat", "Yes", "No"), levels = c("Yes", "No")),
         did_it_rain = factor(ifelse(rainfall == 0.0, "No", "Yes"), levels = c("Yes","No")),
         did_it_snow = factor(ifelse(snowfall == 0.0, "No", "Yes"), levels = c("Yes", "No")),
         month = factor(lubridate::month(date, label = TRUE), levels = c("Dec", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov")),
         week = lubridate::epiweek(date),
         week = ifelse(month != "Dec" | (month == "Dec" & week == 1), week+52, week),
         week = week - 48,
         year = lubridate::year(date)
         )

# split data

set.seed(151020)

seoul_bike_split <- seoul_bike %>% 
  initial_split(prop = 0.8)

bike_train <- training(seoul_bike_split)
bike_test <- testing(seoul_bike_split)
bike_folds <- bike_train %>% vfold_cv(v = 5, repeats = 3)

```

```{r, eval = F}
write_csv(seoul_bike, file = "cleaned_data.csv")
write_csv(bike_train, file = "training_data.csv")
write_csv(bike_test, file = "testing_data.csv")

```


## Introduction

The purpose of this project is to generate a model that will predict under which circumstance individuals are most likely to rent bikes through a bike-share program in Seoul, South Korea. This kind of model could be useful for bike-share companies to better manage their bicycle supply to most adequately support their consumer base.

What is a bike-share program? To those unaware, a bike-share program is a service in which bicycles are available for shared use to individuals for a fee. Bike share companies across the globe have been popping up at an exhausting rate. Locally, several different companies have *attempted* to sell their product on UCSB campus, namely HOPR and LimeBike, over the past few years. I can imagine that in a place like Seoul, where the population heavily relies on public transit, bike-share companies can be more profitable.

The data I use is from UCI's Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand) which joins the count of public bikes rented at each hour in Seoul Bike Sharing System with the corresponding weather data and Holidays information from the following sources: https://data.seoul.go.kr/ and https://publicholidays.go.kr. The data spans from December 1st 2017 to November 30th 2018 (total of 8760 observations) and includes holiday and weather attributes. Specifically, their data set came with the following attributes:

- Date mm/dd/yyyy
- Rented Bike Count
- Hour (of the day from 0 to 23)
- Temperature (Celsius)
- Humidity (%)
- Wind Speed (m/s)
- Visibility (10m)
- Dew point temperature (Celsius)
- Solar Radiation (MJ/m2)
- Rainfall (mm)
- Snowfall (cm)
- Seasons (Winter, Summer, Spring, or Autumn)
- Holiday (Yes or No) 
- Functioning Day (Yes or No) 


I added a few additional attributes of interest for data exploration:

- Time of day (Night, Morning, Afternoon, or Evening based on value of Hour)
- Weekday (factor value 1:7 corresponding to day of the week where 1 is Sunday, 2 is Monday, etc)
- Workday (Yes or No where Mon-Fri are workdays)
- Month (Jan through Dec)
- Week (0 to 52, where week 0 is the first week of the dataset and 52, the last)
- did_it_rain (Yes if rainfall > 0.0, else No)
- did_it_snow (Yes if snowfall > 0.0, else No)

Minimal data cleaning was executed on this data set. I removed special characters from attribute names ($^\circ$, etc.) so the data could be read by R. Then, I filtered out the Non-functioning days since no information is collected about bike rentals. And I additionally reformatted the attribute names to snake_case, converted Date to date format, adjusted humidity to be values in (0,1), and changed attributes from character/numeric to factor type where necessary.


### Data Split

Before doing any analysis of the data, I conducted an 80:20 training:testing split resulting in a training set with 7008 observations and a testing set with 1752 observations, both adequate for model building. The split is not stratified on any variable since the variables we are most concerned with are continuous. I also created folds from the training set with 5 partitions and 3 repeats.


## Data Exploration

How do these attributes interact with each other? How do weather, type of day, time of day, and season affect how many bikes are rented?

To see how different weather indicators interact with the amount of bikes rented, we can look at the correlations between numeric variables:

```{r}
bike_train %>% 
  select(where(is.numeric), -week, -year) %>% 
  cor(use="pairwise.complete.obs") %>% 
  corrplot(type = 'lower', diag = FALSE, method = 'color', tl.col = 'black')
```

As we can see, the rented bike count has a positive correlation with temperature, wind speed, visibility, dew-point-temperature, and solar radiation. And is negatively correlated with humidity, rainfall, and snowfall. This implies that more people want to rent bikes when temperature and visibility are high and humidity, rainfall, and snowfall are low --- people are more likely to rent bikes when the weather is good.

Let's look at some of these relationships in greater detail:

```{r}
bike_train %>% 
  ggplot(aes(temperature, rented_bike_count)) + geom_point(alpha = 0.1) + geom_smooth()
```

Now we get a better picture of how temperature affects rented bike count. Following the trend line in blue, we see that rented bike count steadily increases as temperature goes from $-20^\circ$C to about $30^\circ$C before declining as it gets hotter.

```{r}
bike_train %>% 
  ggplot(aes(humidity, rented_bike_count)) + geom_point(alpha = 0.1) + geom_smooth()
```

We see that rented bike count takes a significant dip when humidity increases above 80%.

```{r}
bike_train %>%
  ggplot(aes(rainfall, rented_bike_count)) + geom_point(alpha = 0.3) + geom_smooth(method = "lm")
```

Rainfall appears to affect the amount of bikes rented very significantly, but maybe this is simply because there are more days with no rain (rainfall = 0.0), thus more bikes are rented?

```{r}
bike_train %>% 
  ggplot(aes(did_it_rain, rented_bike_count)) + geom_boxplot()
```

Clearly this is the case, so if we filter to only include positive values of rainfall, we see that there is really only a slight decrease as rainfall increases.


```{r}
bike_train %>% filter(rainfall != 0.0) %>% 
  ggplot(aes(rainfall, rented_bike_count)) + geom_point(alpha = 0.3) + geom_smooth(method = "lm")
```

We can apply the same logic to snowfall and see that while overall more bikes are rented on days where it is not snowing, there is minimal decrease in amount of rented bikes as snowfall increases.

```{r}
bike_train %>% 
  ggplot(aes(did_it_snow, rented_bike_count)) + geom_boxplot()

bike_train %>% filter(snowfall != 0.0) %>%
  ggplot(aes(snowfall, rented_bike_count)) + geom_point(alpha = 0.5) + geom_smooth(method = "lm")
```


Now lets look at how rented bike count is affected time-based attributes: date, month, season, weekday, hour, time of day, workday and holidays.

Compare how rented bike count is affected over the course of the year:

```{r}
bike_train %>% 
  ggplot(aes(date, rented_bike_count)) + geom_point() + geom_smooth()
```

From this first plot, we see that there's a dip in winter and spike in the early summer.

Another way to visualize how many bicycles are rented throughout the year is through this heat map:

```{r}
bike_train %>% 
  ggplot(aes(week, weekday, fill = rented_bike_count)) + 
  geom_tile(color = "white") +
  scale_fill_gradient(low="red", high="green")

```

Where each box corresponding to a day is colored by the average number bikes rented per hour (empty boxes are from non-functional days). 

Another heat map comparing the day of week to month:

```{r}
bike_train %>% 
  ggplot(aes(month, weekday, fill = rented_bike_count)) + 
  geom_tile(color = "white") +
  scale_fill_gradient(low="red", high="green")
```

We see that there are hotspots on Fridays in May, Thursdays in June and others.

And these next few plots confirm that the summer months are by far the most popular time to rent bikes and the winter months are the least.

```{r}

month_bp <- bike_train %>% 
  ggplot(aes(rented_bike_count, month)) + geom_boxplot() + scale_y_discrete(limits=rev)
month_col <- bike_train %>% 
  ggplot(aes(rented_bike_count, month)) + geom_col() + scale_y_discrete(limits=rev)
seasons_bp <- bike_train %>% 
  ggplot(aes(rented_bike_count, seasons)) + geom_boxplot() + scale_y_discrete(limits=rev)
seasons_col <- bike_train %>% 
  ggplot(aes(rented_bike_count, seasons)) + geom_col() + scale_y_discrete(limits=rev)

grid.arrange(month_bp, month_col, seasons_bp, seasons_col, ncol = 2)
```

Finally, lets look at how working and holidays affect rented bike counts:

```{r}
workday_bp <- bike_train %>% 
  ggplot(aes(workday, rented_bike_count)) + geom_boxplot()

holiday_bp <- bike_train %>% 
  ggplot(aes(holiday, rented_bike_count)) + geom_boxplot()

grid.arrange(workday_bp, holiday_bp, ncol = 2)

```

Slightly more bikes are rented on workdays compared to weekends. Similarly, more bikes are rented on non-holidays compared to holidays. A reasonable explanation for this is that fewer people are biking on days that they typically have off from work.


Then, during a single day we can compare the frequency at which bikes are rented by hour and by time of day.

```{r}
bike_train %>% 
  ggplot(aes(hour, rented_bike_count)) + geom_col()

bike_train %>% 
  ggplot(aes(time_of_day, rented_bike_count)) + geom_boxplot()
```

6pm is the most popular hour to rent bikes, while 4am is the least. More bikes are rented in the evening (between 6pm and midnight) than any other time of day. And fewer rented during nighttime (midnight to 6am).


## Model Building

For this regression problem, we will fit 4 different models (Linear Regression, Regularized Regression, Boosting and Random Forest Models), calculate pertinent metrics from each, and determine which one is best at predicting rented bike counts.

Fist we build, our recipe. Since several of the attributes in the data set are directly correlated (i.e. workday and weekday, season and month) I chose the variables that give more information (i.e. weekday, month) to be predictors in the recipe. Then, deal with factor variables with step_dummy and step_novel. Finally, center and scale all numeric predictors.

```{r}
# letsa get cookin
bike_recipe <- recipe(rented_bike_count ~
                         hour +
                         temperature +
                         humidity +
                         wind_speed +
                         visibility +
                         dew_point_temperature +
                         solar_radiation +
                         rainfall +
                         snowfall +
                         holiday +
                         month +
                         weekday,
                       data = bike_train) %>% 
  step_novel(hour, holiday, month, weekday) %>% 
  step_dummy(hour, holiday, month, weekday) %>% 
  step_normalize(all_numeric_predictors())

```


### Linear Regression Model

I first chose to fit a Linear regression model to the data

```{r}

# linear regression model

lm_model <- linear_reg() %>% 
  set_engine("lm")

lm_workflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(bike_recipe)

lm_fit <- fit(lm_workflow, bike_train)

lm_bike_train_res <- predict(lm_fit, new_data = bike_train %>% select(-rented_bike_count)) %>% 
  bind_cols(bike_train %>% select(rented_bike_count))



bike_metrics <- metric_set(rmse, rsq, mae)
bike_metrics(lm_bike_train_res, truth = rented_bike_count, estimate = .pred)

```

### Regularized Regression -- Elastic Net Model

```{r}

elastic_net_spec <- 
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

elastic_net_wf <- workflow() %>% 
  add_recipe(bike_recipe) %>% 
  add_model(elastic_net_spec)

elastic_net_grid <- grid_regular(mixture(range = c(0, 1)), penalty(range = c(-5, 5)), levels = 10)
```



```{r, eval = F}
elastic_net_tune_res <- tune_grid(
  elastic_net_wf,
  resamples = bike_folds, 
  grid = elastic_net_grid
)

write_rds(elastic_net_tune_res, file = "elastic-net-res.rds")
```

```{r}
elastic_net_tune_res_read <- read_rds(file = "elastic-net-res.rds")

```


### Random Forest Model

```{r}

tree_bike_spec <- decision_tree() %>% 
  set_engine("rpart")

reg_tree_bike_spec <- tree_bike_spec %>% 
  set_mode("regression")

reg_tree_bike_fit <- reg_tree_bike_spec %>% 
  fit(rented_bike_count ~ hour + temperature +  humidity +  wind_speed + visibility + dew_point_temperature + solar_radiation + rainfall + snowfall + holiday + month + weekday,
                       data = bike_train)

reg_tree_bike_wf <- workflow() %>% 
  add_model(reg_tree_bike_spec %>% set_args(cost_complexity = tune())) %>% 
  add_recipe(bike_recipe)

```

```{r, eval = F}
bike_folds <- bike_train %>% vfold_cv(v = 5, repeats = 3)
param_grid <- grid_regular(cost_complexity(range = c(-5, 5)), levels = 50)

reg_tree_bike_tune_res <- tune_grid(
  reg_tree_bike_wf, 
  resamples = bike_folds, 
  grid = param_grid
)

write_rds(reg_tree_bike_tune_res, file = "reg-tree-res.rds")
```


```{r}
reg_tree_bike_tune_res_read <- read_rds(file = "reg-tree-res.rds")
```

### Boosted Tree Model

```{r}
boosted_tree_spec <- boost_tree(min_n = tune(),
                       mtry = tune(),
                       learn_rate = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost")

boosted_tree_wf <- workflow() %>% 
  add_model(boosted_tree_spec) %>% 
  add_recipe(bike_recipe)

param_grid <- grid_regular(min_n(range = c(1, 10)),
                           mtry(range = c(2, 12)), 
                           learn_rate(range = c(-5, 5)),
                           levels = 50)


```




```{r, eval = F}
boosted_tree_tune <- boosted_tree_wf %>% 
  tune_grid(
    resamples = bike_folds, 
    grid = param_grid
    )

write_rds(boosted_tree_tune, file = "boosted_tree_tune.rds")
```

This took a crazy amount of time to run so I wrote out the results

```{r}
boosted_tree_tune_read <- read_rds(file = "boosted_tree_tune.rds")
```


## Implementing the Best Model on the Testing set

To find which model performed best on the training data, we compare rmse values. First let's choose each model for the best tuning parameters.

```{r}

# rmse for linear model

bike_metrics <- metric_set(rmse)
bike_metrics(lm_bike_train_res, truth = rented_bike_count, estimate = .pred)

# finalize workflow for best en model

best_penalty_en <- select_best(elastic_net_tune_res_read, metric = "rmse")

elastic_net_final <- finalize_workflow(elastic_net_wf, best_penalty_en)

elastic_net_final_fit <- fit(elastic_net_final, data = bike_train)

augment(elastic_net_final_fit, new_data = bike_train) %>%
  rmse(truth = rented_bike_count, estimate = .pred)

# finalize workflow for best random forest model

best_penalty_rt <- select_best(reg_tree_bike_tune_res_read, metric = "rmse")

reg_tree_final <- finalize_workflow(reg_tree_bike_wf, best_penalty_rt)

reg_tree_final_fit <- fit(reg_tree_final, data = bike_train)

augment(reg_tree_final_fit, new_data = bike_train) %>%
  rmse(truth = rented_bike_count, estimate = .pred)

# finalize workflow for best boosted tree model

best_penalty_bt <- select_best(boosted_tree_tune_read, metric = "rmse")

boosted_tree_final <- finalize_workflow(boosted_tree_wf, best_penalty_bt)

boosted_tree_final_fit <- fit(boosted_tree_final, data = bike_train)

augment(boosted_tree_final_fit, new_data = bike_train) %>%
  rmse(truth = rented_bike_count, estimate = .pred)

```

The random forest performs the best on the training data with an rmse of 188.3. Let's compare to how it performs on the testing data:

```{r}
augment(reg_tree_final_fit, new_data = bike_test) %>%
  rmse(truth = rented_bike_count, estimate = .pred)
```

Yikes! Overfitting! The rmse has gone up significantly on the testing data.

Regardless, since the boosted tree model has the lowest rmse metric, it is the best candidate for fitting the dataset out of our potential models. Is it the best possible model? Probably not.

```{r}
lm_bike_train_res 

```





## Code appendix

```{r appendix, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```